---
title: K近邻法学习笔记
mathjax: true
date: 2023-03-13 14:42:55
categories: 机器学习
description: k近邻法（KNN）是一种基本分类与回归方法，本文只讨论分类问题中的KNN法
---

## 定义

KNN的定义十分简单粗暴：对于一个待分类实例，在训练集中寻找$k$个距离最近的数据，这$k$个实例多数属于哪一类那么这个待分类实例就属于哪一类。

此处我们说的距离不仅仅是通常为大家熟知的欧氏距离，也有更一般的$L_P$距离，其定义如下：

若存在两个$n$维向量$x_i,x_j$，则$L_p(x_i,x_j)=(\sum_l^{n}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$这里$p\ge1$

当$p=2$时，$L_p$距离称为欧氏距离，即$L_2(x_i,x_j)=\sqrt{\sum_l^{n}|x_i^{(l)}-x_j^{(l)}|^2}$

当$p=1$时，$L_p$距离称为曼哈顿距离，即$L_1(x_i,x_j)=\sum_l^{n}|x_i^{(l)}-x_j^{(l)}|$

当$p=\infty$时，$L_p$距离称为切比雪夫距离，即$L_\infty(x_i,x_j)=\max_{l}|x_i^{(l)}-x_j^{(l)}|$

对于$L_p$距离来说，$p$取值的不同会导致$k$近邻点选择的不同。

## $K$值的选择

$k$值的选择会对$k$近邻法的结果产生重大影响。

如果选择较小的$k$值，那么学习的近似误差（approximation error）会减小，而估计误差（estimation error）会增大，换言之会使得模型变复杂，容易发生过拟合。

$k$取值较大的情况与上文正好相反。

我们常采用交叉验证法来选取最合适的$k$值。

## $K$近邻法的实现——KDTree

在算法竞赛中曾看到过KDTree的影子，可惜我太菜了就没有学，下面放一个详解链接：

<https://oi-wiki.org/ds/kdt/>

值得注意的是，平衡的KDTree在搜索时的效率不一定是最优的，并且KDTree比较适合训练实例数远大于空间维数时的$k$近邻法，当二者相近时其效率会迅速下降，几乎接近线性扫描。
