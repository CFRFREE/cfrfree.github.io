---
title: 分词算法学习笔记1
date: 2023-02-18 22:11:44
mathjax: true
categories: NLP
description: 寒假学习了Python中结巴分词的算法，在这里简单记录一下存在登录词时的结巴分词原理。
---

## 流程概述

1. 根据已有的登陆词信息构建登陆词前缀Trie树
2. 对于每一个即将要分词的句子按照前缀Trie树上的信息建一张有向无环图
3. 在有向无环图上跑DP得到最大概率路径
4. 按照最大概率路径进行划分

## 一、根据已有的登陆词信息构建登陆词前缀Trie树

登录词以（词语，频数，词性）的方式存储在dict.txt中，比如（苏州大学，23，nt），本篇博客中我们只需要使用前两者。

Trie树是一种基础的数据结构，这里就不过多赘述了。

以“苏州大学”这个词为例，我们需要遍历其所有前缀得到“苏”，“苏州”，“苏州大”，“苏州大学”并在Trie上记录对应的频数（没有就是0）。值得注意的是，此时我们构建的Trie树并不是像平常一样使用结构体保存，而是十分地暴力地放到一个字典中，据说可以提高程序效率。这种做法从时空复杂度角度考虑无疑是不正确的，但考虑到汉字登录词长度一般较小，所以尚在可以接受的范围之内。

在建树的同时，我们记录所有登录词的频数之和，这样每个登录词的出现频率也就是其出现频数除以这个频数之和。

## 二、对每个句子建有向无环图

对于需要我们进行分词的句子，我们可以根据Trie树中的信息对频数大于0的所有子串建一条权值等于频数的有向边，这样我们在下面进行DP的时候就可以不考虑后效性了（类似于先缩点再拓扑排序最后DP的套路）。

以“苏州大学”为例，我们会从“苏”节点分别向“州”和“学”两个节点以及从“大”节点到“学”节点建边。

## 三、在DAG上使用动态规划求最大概率路径

我们使用$DP[i]$表示从第$i$位汉字到末尾这样一个子串的最大划分概率，那么对于所有能到达$i$点的点$j$来说，如果$j$到$i$这条边的权重为$w$，那么$DP[j]=\max(DP[j], DP[i]\times w)$，同时记录下$j$是从哪个点转移的。

注意到这种朴素的求法会进行很多次浮点数相乘，为了防止溢出我们对所有数值取对数，以加减操作代替乘除操作。

这样我们便可以依据转移过程对原句子进行分词了！

## 总结

没想到分词算法竟然如此的简单巧妙，但有人会问如果不存在给定的登录词字典该怎么办，这个问题将在下一篇文章中进行解释。
