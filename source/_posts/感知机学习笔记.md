---
title: 感知机学习笔记
mathjax: true
date: 2023-03-13 10:36:21
categories: 机器学习
description: 感知机是一种简单（并不）的二类分类的线性分类模型
---

## 定义

$$
\begin{cases}
f(x)=sign(w\cdot x+b)\\\\
sign(x)=
\begin{cases}
+1,x\ge0\\
-1,x<0
\end{cases}
\end{cases}
$$

其中的$f(x)$称为感知机，$w$叫做权值或权值向量，$b$叫做偏置，$w\cdot x$是两个向量的内积

如果对于数据集$T=\set{(x_i,y_i)},y_i\in\set{-1,1}$，如果存在某个超平面$S:w\cdot x+b=0$使得数据集中所有正实例点和负实例点完全正确地划分到超平面的两侧，则称数据集$T$为线性可分。

## 感知机的损失函数

很自然地，我们会把分类错误的点的个数当作损失函数，然而这种损失函数不是参数$w,b$的连续可导函数，难以优化，所以我们选择所有误分类点到超平面$S$的距离的和作为损失函数。而空间中任一点$x_0$到超平面$S$的距离为$\cfrac{|w\cdot x_0+b|}{||w||}$，这里的$||w||=\sqrt{\sum w_i^2}$，即$w$的$L_2$范数。

进一步地，对任意一个误分类点来说，$y_i(w\cdot x_i+b)$异号（这个式子又被叫做样本点的函数间隔），因此可以用$-\cfrac{1}{||w||}\sum y_i(w\cdot x_i+b)$来作为整个感知机的损失函数。

为了方便计算，我们忽略掉前面的$\cfrac{1}{||w||}$，因为我们更关心$w$的方向，而不是其大小，因此可以将$||w||$当作$1$。

于是我们得到$L(w,b)=-\sum y_i(w\cdot x_i+b)$。

## 感知机学习算法

对于感知机问题，我们使用随机梯度下降法进行解决，首先选取一个随机的超平面$(w_0,b_0)$，然后使用梯度下降法不断极小化$L(w,b)$，这里我们使用的梯度下降并不是一次使数据集中所有的误分类点的梯度下降，而是随机选取一个误分类点。下面给出了$L(w,b)$的梯度：
$$
\begin{cases}
\nabla_wL(w,b)=-\sum y_ix_i\\\\
\nabla_bL(w,b)=-\sum y_i
\end{cases}
$$
随机选取一个误分类点$(x_i,y_i)$，对$w$和$b$进行更新：
$$
\begin{cases}
w=w-\eta\nabla_w\\\\
b=b-\eta\nabla_b
\end{cases}
$$
即
$$
\begin{cases}
w=w+\eta\sum y_ix_i\\\\
b=b+\eta\sum y_i
\end{cases}
$$
这样可以使得$L(w,b)$不断减小直至$0$，即所有点都分类正确了，根据**Novikoff**定理，当训练集数据线性可分时，感知机学习算法的迭代是收敛的。

## 感知机的对偶问题

观察上文中$w$和$b$的变化过程，我们发现若干次梯度下降可以简化为下式：
$$
\begin{cases}
w=\sum a_iy_ix_i\\\\
b=\sum a_iy_i
\end{cases}
$$
其中$a_i$的大小和了第$i$个点被更新的次数呈正比，当$\eta=1$时二者相等。

既然如此我们可以尝试下面的感知机学习方式：

1. 初始化$a,w,b$

2. 对于数据$(x_i,y_i)$

3. 如果$y_i(wx_i+b)\leq0$即$y_i(\sum_j^N a_jy_jx_j\cdot x_i+b)\leq0$

    就进行如下更新：
    $$
    \begin{cases}
    a_i=a_i+\eta\\\\
    b=b+\eta y_i
    \end{cases}
    $$
    
4. 直到不存在误分类点

在第三步，我们多次进行了$x_j\cdot x_i$的计算，这里可以先预处理出所有的内积并以矩阵的形式存放，这就是所谓的**Gram**矩阵。
$$
G=[x_i\cdot x_j]_{N\times N}
$$
